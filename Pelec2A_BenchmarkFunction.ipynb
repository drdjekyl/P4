{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, pipeline\n",
    "from sklearn import kernel_ridge, svm\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoCV, ElasticNet, Ridge\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import metrics\n",
    "from math import log\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "\n",
    "# Import all lib to create benchmark functions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The differents models are created with the same pattern\n",
    " - Function with no arg\n",
    " - Grid param in the body function -> Grid Search CV in Benchmark Notebook\n",
    " - Return a dictionnary with model name, model function, grid param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modèle 0 : Dummy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dum():\n",
    "    grid_param0 = {\n",
    "        'dummy__strategy': ['mean']   \n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'model_name': 'dummy',\n",
    "        'model': DummyRegressor(),\n",
    "        'param': grid_param0\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modèle 1 : Linear regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg():\n",
    "    grid_param1 = {}\n",
    "    \n",
    "    return {\n",
    "        'model_name': 'linear_regression',\n",
    "        'model': LinearRegression(),\n",
    "        'param': grid_param1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modèle 2 : Forêt aléatoire**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ran_for(n, d, f, leaf, i):\n",
    "    grid_param2 = {\n",
    "        'random_forest__n_estimators': n,\n",
    "        'random_forest__max_depth': d,\n",
    "        'random_forest__max_features': f,\n",
    "        'random_forest__min_samples_leaf': leaf,\n",
    "        'random_forest__min_impurity_decrease': i\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'model_name': 'random_forest',\n",
    "        'model': RandomForestRegressor(),\n",
    "        'param': grid_param2\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modèle 3 : Gradient Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_boo(n, d, f):\n",
    "    grid_param3 = {\n",
    "        'gradient_boosting__n_estimators': n,\n",
    "        'gradient_boosting__max_depth': d,\n",
    "        'gradient_boosting__max_features': f,\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'model_name': 'gradient_boosting',\n",
    "        'model': GradientBoostingRegressor(),\n",
    "        'param': grid_param3\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modèle 4 : XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xg_boo(n, d, l, b):\n",
    "    grid_param = {\n",
    "        'xgboost__n_estimators': n,\n",
    "        'xgboost__max_depth': d,\n",
    "        'xgboost__learning_rate': l,\n",
    "        'xgboost__booster': b\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'model_name': 'xgboost',\n",
    "        'model': xgb.XGBRegressor(),\n",
    "        'param': grid_param\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modèle 5 : Lasso**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso(alphas):\n",
    "    grid_param = {\n",
    "        'lasso__alpha': alphas,\n",
    "        'lasso__fit_intercept': [True, False],\n",
    "        'lasso__max_iter': [100000]\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'model_name': 'lasso',\n",
    "        'model': Lasso(),\n",
    "        'param': grid_param\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modèle 6 : ElasticNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ela_net(alphas, l1):\n",
    "    grid_param = {\n",
    "        'elastic__alpha': alphas,\n",
    "        'elastic__l1_ratio': l1,\n",
    "        'elastic__fit_intercept': [True, False],\n",
    "        'elastic__max_iter': [200000],\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'model_name': 'elastic',\n",
    "        'model': ElasticNet(),\n",
    "        'param': grid_param\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modèle 7a : Regression Ridge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge(alphas, s):\n",
    "    grid_param = {\n",
    "        'ridge__alpha': alphas,\n",
    "        'ridge__fit_intercept': [True, False],\n",
    "        'ridge__max_iter': [100000],\n",
    "        'ridge__solver': s,\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'model_name': 'ridge',\n",
    "        'model': Ridge(),\n",
    "        'param': grid_param\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modèle 7b : Regression Ridge (kernel version)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_kern(alphas, gammas, kern):\n",
    "    grid_param = {\n",
    "        'ridge_kernel__alpha': alphas,\n",
    "        'ridge_kernel__gamma': gammas,\n",
    "        'ridge_kernel__kernel': kern,\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'model_name': 'ridge_kernel',\n",
    "        'model': kernel_ridge.KernelRidge(),\n",
    "        'param': grid_param\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modèle 8 : Support Vector Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svr(c, tol):\n",
    "    grid_param = {\n",
    "    'svr__loss': ['squared_epsilon_insensitive'],\n",
    "    'svr__C': c,\n",
    "    'svr__fit_intercept': [True],\n",
    "    'svr__max_iter': [100000],\n",
    "    'svr__tol': tol\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'model_name': 'svr',\n",
    "        'model': svm.LinearSVR(),\n",
    "        'param': grid_param\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmark des modèles crées**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T16:28:44.356729Z",
     "start_time": "2020-12-03T16:28:44.341057Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_benchmark(x, y, reg, models):\n",
    "    \"\"\"Function to compare a list of model with a dataset\n",
    "\n",
    "    Args :\n",
    "    x : arrays or pd.dataFrame\n",
    "    y : one dimension array or pd.Series\n",
    "    scal : none, norm or log may be put in argument \n",
    "           -> scale dataset\n",
    "    models : a list of model included in the model_benchmark could be chose\n",
    "\n",
    "    Return : \n",
    "    a dataframe with best param, mse and r square for each model tested\n",
    "\n",
    "    \"\"\"\n",
    "    # Create a dF to stock results of each model by column\n",
    "    model_performance = pd.DataFrame(dtype='float64')\n",
    "\n",
    "    # List to stock param of each model\n",
    "    param = []\n",
    "    elaps_time = []\n",
    "    yt = []\n",
    "    yp = []\n",
    "\n",
    "    # Split data in train and test\n",
    "    x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
    "        x, y, test_size=0.3, random_state=0)\n",
    "\n",
    "    # Define the scaler\n",
    "    std_scale = preprocessing.StandardScaler()\n",
    "    normalizer = preprocessing.Normalizer(norm='l2')  # fit does nothing\n",
    "\n",
    "    a = np.logspace(-5, 5, 400)\n",
    "\n",
    "    for i in models:\n",
    "        model = i\n",
    "\n",
    "        # Start timer\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        # Scaling operation within the pipeline selected in arg\n",
    "        if reg == 'lasso':\n",
    "            pipe = pipeline.Pipeline(steps=[(\n",
    "                'scaler',\n",
    "                std_scale), ('weighter', SelectFromModel(LassoCV(\n",
    "                    alphas=a))), (i['model_name'], i['model'])])\n",
    "        elif reg == 'norm':\n",
    "            pipe = pipeline.Pipeline(\n",
    "                steps=[('norm', normalizer), (i['model_name'], i['model'])])\n",
    "        else:\n",
    "            pipe = pipeline.Pipeline(\n",
    "                steps=[('scaler', std_scale), (i['model_name'], i['model'])])\n",
    "\n",
    "        # Grid search with pipeline and grid param in each model function\n",
    "        grids = model_selection.GridSearchCV(\n",
    "            pipe, i['param'], cv=10, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "        # Fit the model / then predict y value\n",
    "        grids.fit(x_train, y_train)\n",
    "        y_pred = grids.predict(x_test)\n",
    "\n",
    "        # Time stop\n",
    "        time_stop = timeit.default_timer() - start_time\n",
    "\n",
    "        # Best params in training\n",
    "        param.append([i['model_name'], grids.best_params_])\n",
    "        yt.append([i['model_name'], y_test])\n",
    "        yp.append([i['model_name'], y_pred])\n",
    "        elaps_time.append([i['model_name'], str(time_stop)])\n",
    "\n",
    "        # Scoring with RMSE train and test, R2 test\n",
    "        train_rmse = (grids.best_score_) * (-1)\n",
    "        test_rmse = (metrics.mean_squared_error(y_test, y_pred, squared=False))\n",
    "        r2 = metrics.r2_score(y_test, y_pred)\n",
    "\n",
    "        # Create a dF to display all the score results obtained by ML model\n",
    "        model_serie = pd.DataFrame(\n",
    "            {\n",
    "                i['model_name']: [\n",
    "                    train_rmse.round(3),\n",
    "                    test_rmse.round(3),\n",
    "                    r2.round(3), time_stop\n",
    "                ]\n",
    "            },\n",
    "            index=['Train RMSE', 'Test RMSE', 'R²', 'Time'],\n",
    "            dtype='float64')\n",
    "        model_performance = pd.concat([model_serie, model_performance], axis=1)\n",
    "\n",
    "        # Check progression of the model fitted during running of the function\n",
    "\n",
    "        print(\"{} is trained and fitted in {:.3f} sec\".format(\n",
    "            i['model_name'], time_stop))\n",
    "        print(grids.cv_results_)\n",
    "\n",
    "    # Output dict\n",
    "    return {\n",
    "        'set': y.name,\n",
    "        'perf': model_performance,\n",
    "        'param': param,\n",
    "        'time': elaps_time,\n",
    "        'yt': yt,\n",
    "        'yp': yp\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the perf of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_yt_yp(model_name, y_test, y_pred, rmse, r2, lim):\n",
    "    # Fig param\n",
    "    fig = plt.figure(figsize=(7, 7))\n",
    "\n",
    "    # Define size of circle for yt, yp\n",
    "    sizes = {}\n",
    "    for (yt, yp) in zip(y_test, y_pred):\n",
    "        if (yt, yp) in sizes.keys():\n",
    "            sizes[(yt, yp)] += 1\n",
    "        else:\n",
    "            sizes[(yt, yp)] = 1\n",
    "    keys = sizes.keys()\n",
    "\n",
    "    # Plot the prediction\n",
    "    plt.scatter([k[0] for k in keys], [k[1] for k in keys],\n",
    "                s=[sizes[k] for k in keys],\n",
    "                label=[\"RMSE = {:.2f}\".format(rmse), \"R² = {:.2f}\".format(r2)])\n",
    "\n",
    "    # Set title, x and y labels\n",
    "    plt.xlabel('True score', fontsize=16)\n",
    "    plt.ylabel(u'Predicted Score', fontsize=16)\n",
    "    plt.title(f'Best model {model_name}', fontsize=16)\n",
    "    \n",
    "    # x and y lim\n",
    "    plt.xlim(0, lim)\n",
    "    plt.ylim(0, lim)\n",
    "\n",
    "    plt.legend(loc=\"lower right\", fontsize=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
